---
title: "Heat Vulnerability Index Analysis - Johannesburg"
author: "Craig Parker"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
# Function to install and load required packages
setup_packages <- function(packages) {
  for(pkg in packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
      message(paste("Installing package:", pkg))
      install.packages(pkg, repos = "https://cran.rstudio.com/")
    }
    library(pkg, character.only = TRUE)
  }
}

# List of required packages
required_packages <- c(
  "knitr", "rmarkdown", "GWmodel", "sp", "sf", "dplyr",
  "ggplot2", "viridis", "corrplot", "rlang", "psych",
  "tidyr", "spdep", "RColorBrewer", "factoextra", "mice",
  "tmap", "tmaptools"
)

# Install and load packages
tryCatch({
  setup_packages(required_packages)
}, error = function(e) {
  stop("Error setting up required packages: ", e$message)
})

# Set knitr options
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  cache = FALSE  # Default cache setting
)

# Create cache directory with error handling
cache_dir <- "cache"
tryCatch({
  if (!dir.exists(cache_dir)) {
    dir.create(cache_dir, recursive = TRUE)
    print(paste("Created cache directory:", cache_dir))
  } else {
    print(paste("Cache directory already exists:", cache_dir))
  }
  
  # Test write permissions
  test_file <- file.path(cache_dir, "test.txt")
  writeLines("test", test_file)
  unlink(test_file)
  print("Cache directory is writable")
  
}, error = function(e) {
  warning(paste("Error with cache directory:", e$message))
  print("Will proceed without caching")
})

# Set cache directory for knitr
knitr::opts_chunk$set(cache.path = file.path(cache_dir, "/"))

# Create figures directory if it doesn't exist
figures_dir <- "figures"
if (!dir.exists(figures_dir)) {
  dir.create(figures_dir)
  print(paste("Created figures directory:", figures_dir))
}

# Function to generate timestamped filename
get_timestamped_filename <- function(base_name, ext) {
  timestamp <- format(Sys.time(), "%Y%m%d_%H%M")
  file.path(figures_dir, paste0(base_name, "_", timestamp, ".", ext))
}
```

# Introduction

This notebook implements the heat vulnerability analysis for Johannesburg as detailed in our paper. The analysis integrates environmental, socio-economic, and health data to assess spatial patterns of heat vulnerability across 135 urban wards.

# Load Required Libraries

```{r libraries}
# Load required libraries
library(GWmodel)
library(sp)
library(sf)
library(dplyr)
library(ggplot2)
library(viridis)
library(corrplot)
library(rlang)
library(psych)
library(tidyr)
library(spdep)
library(RColorBrewer)
library(factoextra)
library(mice)
library(tmap)
library(tmaptools)
```

# Data Preparation

## Read and Prepare Data

```{r data-prep, cache=TRUE, cache.path='cache/', dependson=c('libraries')}
# Read data
tryCatch({
  data <- read.csv('data.csv', header = TRUE)
  print("Successfully read data.csv")
  print(paste("Number of rows:", nrow(data)))
  print(paste("Number of columns:", ncol(data)))
}, error = function(e) {
  stop("Error reading data.csv: ", e$message)
})

tryCatch({
  geometry <- st_read("geometry.shp")
  print("Successfully read geometry.shp")
  print(paste("Number of features:", nrow(geometry)))
  print(paste("Original CRS:", st_crs(geometry)$input))
  
  # Transform to UTM zone 35S (Johannesburg)
  geometry <- st_transform(geometry, crs = 32735)
  print("Successfully transformed to UTM Zone 35S")
}, error = function(e) {
  stop("Error reading or transforming geometry.shp: ", e$message)
})

# Ensure consistent IDs
print("Checking ID consistency...")
data$WardID_ <- as.character(data$WardID_)
geometry$WardID_ <- as.character(geometry$WardID_)

# Check for missing IDs
missing_in_data <- setdiff(geometry$WardID_, data$WardID_)
missing_in_geom <- setdiff(data$WardID_, geometry$WardID_)

if(length(missing_in_data) > 0) {
  warning("Some IDs in geometry are missing from data: ", 
          paste(missing_in_data, collapse = ", "))
}
if(length(missing_in_geom) > 0) {
  warning("Some IDs in data are missing from geometry: ", 
          paste(missing_in_geom, collapse = ", "))
}

# Define variables
vars <- c(
  "Crowded.dwellings", "No.piped.water", "Using.public.healthcare.facilities",
  "Poor.health.status", "Failed.to.find.healthcare.when.needed",
  "No.medical.insurance", "Household.hunger.risk",
  "Benefiting.from.school.feeding.scheme", "UTFVI", "LST", "NDVI",
  "NDBI__mean", "concern_he", "cancer_pro", "diabetes_p", "pneumonia_",
  "heart_dise", "hypertensi", "hiv_prop", "tb_prop", "covid_prop",
  "X60_plus_pr"
)

# Check if all variables exist in the data
missing_vars <- vars[!vars %in% names(data)]
if(length(missing_vars) > 0) {
  stop("Missing variables in data: ", paste(missing_vars, collapse = ", "))
}
```

## Handle Missing Data

```{r missing-data, cache=TRUE, cache.path='cache/', dependson=c('data-prep')}
# Merge datasets
merged_data <- merge(geometry[, c("WardID_", "geometry")], data, by = "WardID_", all.x = TRUE)

# Handle missing data using MICE
merged_data_no_geom <- st_drop_geometry(merged_data)
imputed_data <- mice(merged_data_no_geom[vars], m = 5, maxit = 50)
merged_data_complete <- complete(imputed_data)

# Standardize variables
for (var in vars) {
  merged_data[[var]] <- as.numeric(as.character(merged_data[[var]]))
  merged_data[[var]] <- scale(merged_data[[var]])
}
```

# GWPCA Analysis with Data Validation

```{r gwpca_analysis_fixed}
# Load required packages
library(GWmodel)
library(sp)
library(sf)
library(ggplot2)
library(viridis)

# Data validation steps
print("Checking input data dimensions...")
print(paste("Number of rows in merged_data:", nrow(merged_data)))
print(paste("Number of variables:", length(vars)))
print("Variables to be used:")
print(vars)

# Check for missing values
missing_values <- colSums(is.na(merged_data_no_geom[, vars]))
print("\nMissing values per variable:")
print(missing_values)

# Remove any rows with missing values
complete_data <- merged_data_no_geom[complete.cases(merged_data_no_geom[, vars]), vars]
print(paste("\nRows after removing missing values:", nrow(complete_data)))

# Scale the data
data_scaled <- scale(as.matrix(complete_data))
print("\nDimensions of scaled data:")
print(dim(data_scaled))

# Create spatial points with complete data
coords <- st_coordinates(st_centroid(merged_data[complete.cases(merged_data_no_geom[, vars]), ]))
print("\nDimensions of coordinates:")
print(dim(coords))

# Create spatial data frame
sp_data <- SpatialPointsDataFrame(
  coords = coords,
  data = as.data.frame(data_scaled),
  proj4string = CRS(st_crs(merged_data)$wkt)
)

print("\nValidating spatial data frame:")
print(paste("Number of points:", nrow(sp_data)))
print(paste("Number of variables:", ncol(sp_data@data)))

# Calculate bandwidth
bw <- nrow(sp_data) * 0.1
print(paste("\nUsing bandwidth:", bw))

# Run GWPCA with try-catch
print("\nRunning GWPCA...")
gwpca_result <- try({
  gwpca(
    data = sp_data,
    vars = colnames(data_scaled),
    k = 3,
    adaptive = TRUE,
    bw = bw,
    robust = FALSE,
    scores = TRUE
  )
})

if(inherits(gwpca_result, "try-error")) {
  stop("GWPCA failed: ", attr(gwpca_result, "condition")$message)
}

# Extract results
print("\nExtracting GWPCA results...")
var_explained <- gwpca_result$var
prop_var <- var_explained / sum(var_explained)

print("Proportion of variance explained:")
print(prop_var)

# Extract scores
scores_matrix <- matrix(NA, nrow = nrow(sp_data), ncol = 3)
colnames(scores_matrix) <- c("PC1", "PC2", "PC3")

for(i in 1:nrow(sp_data)) {
  if(!is.null(gwpca_result$scores[[i]])) {
    scores_matrix[i,] <- gwpca_result$scores[[i]][1, 1:3]
  }
}

# Calculate HVI
weights <- prop_var[1:3]
hvi_scores <- scores_matrix %*% weights
hvi_standardized <- scale(hvi_scores)[,1]

# Create spatial object with results
gwpca_sf <- st_as_sf(sp_data)
gwpca_sf$PC1 <- scores_matrix[,1]
gwpca_sf$PC2 <- scores_matrix[,2]
gwpca_sf$PC3 <- scores_matrix[,3]
gwpca_sf$HVI <- hvi_standardized

# Save results
saveRDS(gwpca_result, "cache/gwpca_result.rds")
saveRDS(gwpca_sf, "cache/gwpca_sf.rds")

# Basic visualization
print("\nCreating basic visualization...")
png("figures/gwpca_scree_plot.png", width=800, height=600)
barplot(prop_var * 100, 
        names.arg = paste0("PC", 1:length(prop_var)),
        main = "Proportion of Variance Explained by Components",
        ylab = "Variance Explained (%)",
        col = "steelblue")
dev.off()

print("\nGWPCA analysis completed successfully")
```

# Enhanced Visualizations

```{r enhanced_visualizations}
library(ggplot2)
library(viridis)
library(gridExtra)
library(scales)

# 1. Enhanced Scree Plot
scree_data <- data.frame(
  Component = factor(paste0("PC", 1:length(prop_var)), 
                    levels = paste0("PC", 1:length(prop_var))),
  Variance = prop_var * 100
)

p1 <- ggplot(scree_data, aes(x = Component, y = Variance)) +
  geom_bar(stat = "identity", fill = viridis(1, begin = 0.5)) +
  geom_text(aes(label = sprintf("%.1f%%", Variance)), 
            vjust = -0.5, size = 4) +
  labs(title = "Proportion of Variance Explained by Components",
       x = "Principal Component",
       y = "Variance Explained (%)") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text.x = element_text(angle = 0, size = 12),
    axis.text.y = element_text(size = 12),
    panel.grid.major = element_line(color = "gray90"),
    panel.grid.minor = element_line(color = "gray95")
  ) +
  scale_y_continuous(limits = c(0, max(scree_data$Variance) * 1.1))

ggsave("figures/gwpca_scree_plot_enhanced.png", p1, 
       width = 10, height = 7, dpi = 300)

# 2. Enhanced Loadings Plot
loadings_data <- data.frame(
  Variable = factor(colnames(local_loadings), 
                   levels = colnames(local_loadings)[order(mean_loadings)]),
  Loading = mean_loadings
)

p2 <- ggplot(loadings_data, aes(x = Variable, y = Loading)) +
  geom_bar(stat = "identity", 
           fill = colorRampPalette(viridis(3))(nrow(loadings_data))) +
  geom_text(aes(label = sprintf("%.3f", Loading)), 
            hjust = -0.2, size = 3.5) +
  labs(title = "Average Absolute Loadings on PC1",
       x = "Variables",
       y = "Loading Value") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12),
    panel.grid.major = element_line(color = "gray90"),
    panel.grid.minor = element_line(color = "gray95")
  ) +
  coord_flip() +
  scale_y_continuous(limits = c(0, max(loadings_data$Loading) * 1.2))

ggsave("figures/gwpca_loadings_plot_enhanced.png", p2, 
       width = 12, height = 8, dpi = 300)

# 3. Enhanced HVI Distribution
hvi_data <- data.frame(HVI = hvi_standardized)

p3 <- ggplot(hvi_data, aes(x = HVI)) +
  geom_histogram(aes(y = ..density..), 
                bins = 30, 
                fill = viridis(1, begin = 0.2),
                color = "white") +
  geom_density(color = viridis(1, begin = 0.8), 
               linewidth = 1) +
  labs(title = "Distribution of Heat Vulnerability Index",
       x = "Standardized HVI Score",
       y = "Density") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    panel.grid.major = element_line(color = "gray90"),
    panel.grid.minor = element_line(color = "gray95")
  )

ggsave("figures/hvi_distribution_enhanced.png", p3, 
       width = 10, height = 7, dpi = 300)

# 4. Variable Importance Plot
importance_data <- data.frame(
  Variable = names(table(lead_vars)),
  Count = as.numeric(table(lead_vars))
)
importance_data$Percentage <- importance_data$Count / sum(importance_data$Count) * 100
importance_data <- importance_data[order(-importance_data$Count), ]

p4 <- ggplot(importance_data, 
             aes(x = reorder(Variable, Percentage), y = Percentage)) +
  geom_bar(stat = "identity", 
           fill = colorRampPalette(viridis(3))(nrow(importance_data))) +
  geom_text(aes(label = sprintf("%.1f%%", Percentage)), 
            hjust = -0.2, size = 3.5) +
  labs(title = "Most Influential Variables by Location",
       x = "Variables",
       y = "Percentage of Locations (%)") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12),
    panel.grid.major = element_line(color = "gray90"),
    panel.grid.minor = element_line(color = "gray95")
  ) +
  coord_flip() +
  scale_y_continuous(limits = c(0, max(importance_data$Percentage) * 1.2))

ggsave("figures/variable_importance_enhanced.png", p4, 
       width = 12, height = 8, dpi = 300)

# 5. Combined PC1 vs PC2 Plot with HVI
pc_data <- data.frame(
  PC1 = scores_matrix[,1],
  PC2 = scores_matrix[,2],
  HVI = hvi_standardized
)

p5 <- ggplot(pc_data, aes(x = PC1, y = PC2, color = HVI)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_viridis() +
  labs(title = "PC1 vs PC2 Colored by HVI Score",
       x = paste0("PC1 (", sprintf("%.1f", prop_var[1]*100), "% variance)"),
       y = paste0("PC2 (", sprintf("%.1f", prop_var[2]*100), "% variance)")) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    panel.grid.major = element_line(color = "gray90"),
    panel.grid.minor = element_line(color = "gray95"),
    legend.position = "right",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

ggsave("figures/pc_scatter_enhanced.png", p5, 
       width = 10, height = 8, dpi = 300)

# Save all plots in a single figure
combined_plot <- grid.arrange(p1, p2, p3, p4, p5, 
                            ncol = 2, 
                            top = textGrob("GWPCA Analysis Summary", 
                                         gp = gpar(fontsize = 20, fontface = "bold")))

ggsave("figures/gwpca_summary_dashboard.png", combined_plot, 
       width = 20, height = 24, dpi = 300)
```

# Spatial Visualization of HVI
```{r spatial_hvi}
library(tmap)
library(tmaptools)

# Prepare spatial data
gwpca_sf$HVI_cat <- cut(gwpca_sf$HVI, 
                        breaks = c(-Inf, -1.5, -0.5, 0.5, 1.5, Inf),
                        labels = c("Very Low", "Low", "Moderate", "High", "Very High"))

# Create static map
tm_shape(gwpca_sf) +
  tm_fill("HVI_cat", 
          title = "Heat Vulnerability Index",
          palette = viridis(5),
          style = "cat") +
  tm_borders(col = "white", lwd = 0.5, alpha = 0.5) +
  tm_layout(
    title = "Spatial Distribution of Heat Vulnerability Index",
    title.position = c("center", "top"),
    title.size = 1.5,
    frame = FALSE,
    legend.title.size = 1,
    legend.text.size = 0.8,
    legend.position = c("right", "bottom"),
    legend.bg.color = "white",
    legend.bg.alpha = 0.8
  )

# Save high-resolution map
tmap_save(filename = "figures/hvi_spatial_map.png", 
          width = 12, 
          height = 10, 
          dpi = 300)

# Create local Moran's I map for HVI clustering
library(spdep)

# Create neighbors list
nb <- poly2nb(gwpca_sf)
lw <- nb2listw(nb, style = "W")

# Calculate local Moran's I
local_moran <- localmoran(gwpca_sf$HVI, lw)

# Add results to spatial object
gwpca_sf$local_moran <- local_moran[, 1]
gwpca_sf$local_moran_p <- local_moran[, 5]
gwpca_sf$cluster_type <- "Not Significant"
gwpca_sf$cluster_type[gwpca_sf$HVI > 0 & gwpca_sf$local_moran > 0 & gwpca_sf$local_moran_p <= 0.05] <- "High-High"
gwpca_sf$cluster_type[gwpca_sf$HVI < 0 & gwpca_sf$local_moran > 0 & gwpca_sf$local_moran_p <= 0.05] <- "Low-Low"
gwpca_sf$cluster_type[gwpca_sf$HVI > 0 & gwpca_sf$local_moran < 0 & gwpca_sf$local_moran_p <= 0.05] <- "High-Low"
gwpca_sf$cluster_type[gwpca_sf$HVI < 0 & gwpca_sf$local_moran < 0 & gwpca_sf$local_moran_p <= 0.05] <- "Low-High"

# Create LISA cluster map
cluster_colors <- c("High-High" = "#FF0000", 
                   "Low-Low" = "#0000FF",
                   "High-Low" = "#FF69B4",
                   "Low-High" = "#87CEEB",
                   "Not Significant" = "#CCCCCC")

tm_shape(gwpca_sf) +
  tm_fill("cluster_type",
          title = "Local Moran's I Clusters",
          palette = cluster_colors,
          style = "cat") +
  tm_borders(col = "white", lwd = 0.5, alpha = 0.5) +
  tm_layout(
    title = "Spatial Clusters of Heat Vulnerability",
    title.position = c("center", "top"),
    title.size = 1.5,
    frame = FALSE,
    legend.title.size = 1,
    legend.text.size = 0.8,
    legend.position = c("right", "bottom"),
    legend.bg.color = "white",
    legend.bg.alpha = 0.8
  )

# Save LISA cluster map
tmap_save(filename = "figures/hvi_cluster_map.png", 
          width = 12, 
          height = 10, 
          dpi = 300)

# Print summary of clusters
print("Summary of HVI Spatial Clusters:")
print(table(gwpca_sf$cluster_type))

# Calculate Global Moran's I
global_moran <- moran.test(gwpca_sf$HVI, lw)
print("\nGlobal Moran's I Test Results:")
print(global_moran)
```

# Descriptive Statistics

```{r descriptive-stats}
# Compute summary statistics
summary_stats <- merged_data_no_geom %>%
  select(all_of(vars)) %>%
  summarise(across(everything(), list(
    mean = ~mean(., na.rm = TRUE),
    sd = ~sd(., na.rm = TRUE),
    min = ~min(., na.rm = TRUE),
    q25 = ~quantile(., 0.25, na.rm = TRUE),
    median = ~median(., na.rm = TRUE),
    q75 = ~quantile(., 0.75, na.rm = TRUE),
    max = ~max(., na.rm = TRUE)
  )))

# Display summary statistics
knitr::kable(t(summary_stats), digits = 2)
```

# Correlation Analysis

## Test for Normality

```{r normality-test}
# Test for normality
shapiro_results <- lapply(merged_data_no_geom[vars], shapiro.test)
normality_summary <- data.frame(
  Variable = vars,
  Shapiro_p_value = sapply(shapiro_results, function(x) x$p.value)
)

# Display normality test results
knitr::kable(normality_summary, digits = 3)
```

## Compute Correlations

```{r correlations}
# Compute both Pearson and Spearman correlations
pearson_corr <- cor(merged_data_no_geom[vars], 
                    use = "pairwise.complete.obs", 
                    method = "pearson")
spearman_corr <- cor(merged_data_no_geom[vars], 
                     use = "pairwise.complete.obs", 
                     method = "spearman")

# Plot correlation matrices
par(mfrow = c(1, 2))
corrplot(pearson_corr, method = "color", type = "upper", 
         order = "hclust",
         tl.cex = 0.4, tl.col = "black", tl.srt = 45,
         addCoef.col = "black", number.cex = 0.3,
         title = "Pearson Correlation Matrix")
corrplot(spearman_corr, method = "color", type = "upper", 
         order = "hclust",
         tl.cex = 0.4, tl.col = "black", tl.srt = 45,
         addCoef.col = "black", number.cex = 0.3,
         title = "Spearman Correlation Matrix")
```

```{r}
# Calculate updated correlation matrix using Spearman
# First ensure we have all required variables
key_vars <- data.frame(
  LST = merged_data_no_geom$LST,
  NDVI = merged_data_no_geom$NDVI,
  Healthcare_Access = merged_data_no_geom$Using.public.healthcare.facilities,
  Poverty_Index = merged_data_no_geom$Household.hunger.risk
)

# Calculate Spearman correlations
spearman_cor <- cor(key_vars, method = "spearman", use = "pairwise.complete.obs")

# Calculate p-values for correlations
spearman_p <- matrix(NA, 4, 4)
for(i in 1:4) {
  for(j in 1:4) {
    test <- cor.test(key_vars[,i], key_vars[,j], 
                     method = "spearman", 
                     exact = FALSE)
    spearman_p[i,j] <- test$p.value
  }
}

# Format correlation matrix for table
table5_matrix <- round(spearman_cor, 2)
rownames(table5_matrix) <- c("LST", "NDVI", "Healthcare Access", "Poverty Index")
colnames(table5_matrix) <- c("LST", "NDVI", "Healthcare Access", "Poverty Index")

# Add significance stars
table5_matrix_with_stars <- matrix("", 4, 4)
for(i in 1:4) {
  for(j in 1:4) {
    stars <- if(spearman_p[i,j] < 0.001) "***" else if(spearman_p[i,j] < 0.01) "**" else if(spearman_p[i,j] < 0.05) "*" else ""
    table5_matrix_with_stars[i,j] <- paste0(table5_matrix[i,j], stars)
  }
}

# Print formatted table
print("Table 5: Correlation Matrix of Key Variables (Spearman's rho)")
print(table5_matrix_with_stars)

# Save table to CSV
write.csv(table5_matrix_with_stars, "table5_spearman.csv")

# Print correlations with p-values for paper text
cat("\nKey correlations for paper text:\n")
cat("LST & NDVI: r =", table5_matrix[1,2], ", p <", format.pval(spearman_p[1,2], digits = 3), "\n")
cat("LST & Healthcare Access: r =", table5_matrix[1,3], ", p <", format.pval(spearman_p[1,3], digits = 3), "\n")
cat("LST & Poverty Index: r =", table5_matrix[1,4], ", p <", format.pval(spearman_p[1,4], digits = 3), "\n")
cat("Healthcare Access & Poverty Index: r =", table5_matrix[3,4], ", p <", format.pval(spearman_p[3,4], digits = 3), "\n")
```

```{r}
# Get raw percentages for healthcare access
# First unstandardize the data if needed
healthcare_summary <- merged_data %>%
  select(WardID_, Using.public.healthcare.facilities) %>%
  mutate(
    percentage = Using.public.healthcare.facilities * 100,  # Convert to percentage
    WardID_ = as.character(WardID_)
  ) %>%
  arrange(desc(percentage))

# Display top 5 highest usage wards
cat("\nTop 5 wards with highest public healthcare usage:\n")
healthcare_top5 <- healthcare_summary %>%
  slice_head(n = 5) %>%
  mutate(percentage = round(percentage, 2))
print(healthcare_top5)

# Display bottom 5 lowest usage wards
cat("\nBottom 5 wards with lowest public healthcare usage:\n")
healthcare_bottom5 <- healthcare_summary %>%
  slice_tail(n = 5) %>%
  mutate(percentage = round(percentage, 2))
print(healthcare_bottom5)

# Calculate summary statistics
cat("\nSummary Statistics:\n")
cat(sprintf("Mean usage: %.2f%%\n", mean(healthcare_summary$percentage)))
cat(sprintf("Standard deviation: %.2f%%\n", sd(healthcare_summary$percentage)))
cat(sprintf("Range: %.2f%% to %.2f%%\n",
            min(healthcare_summary$percentage),
            max(healthcare_summary$percentage)))

# Create a boxplot to visualize the distribution
ggplot(healthcare_summary, aes(y = percentage)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Distribution of Public Healthcare Usage Across Wards",
       y = "Percentage of Households Using Public Healthcare") +
  theme(plot.title = element_text(hjust = 0.5))
```

# Basic GWPCA Analysis

```{r basic_gwpca}
# Load required packages
library(GWmodel)
library(sp)
library(sf)

# Prepare data
data_matrix <- as.matrix(merged_data_no_geom[, vars])
data_scaled <- scale(data_matrix)

# Create spatial points
coords <- st_coordinates(st_centroid(merged_data))
sp_data <- SpatialPointsDataFrame(
  coords = coords,
  data = as.data.frame(data_scaled),
  proj4string = CRS(st_crs(merged_data)$wkt)
)

# Run basic GWPCA
gwpca_result <- gwpca(
  data = sp_data,
  vars = colnames(data_scaled),
  k = 3,
  adaptive = TRUE,
  bw = nrow(sp_data) * 0.1,  # Using 10% of data points as bandwidth
  robust = FALSE
)

# Print basic results
print("GWPCA Results:")
print(paste("Number of components:", length(gwpca_result$var)))
print("Variance explained:")
print(gwpca_result$var)

# Save results
saveRDS(gwpca_result, "cache/gwpca_basic_result.rds")

# Create simple plot
png("figures/basic_gwpca_plot.png", width=800, height=600)
barplot(gwpca_result$var, 
        names.arg = paste0("PC", 1:length(gwpca_result$var)),
        main = "Variance Explained by Components",
        ylab = "Variance")
dev.off()
```

```{r visualizations, fig.width=12, fig.height=8}
# Create figures directory
if (!dir.exists("figures")) {
  dir.create("figures")
}

# Function for timestamped filenames
get_timestamp_filename <- function(base_name, ext) {
  timestamp <- format(Sys.time(), "%Y%m%d_%H%M")
  file.path("figures", paste0(base_name, "_", timestamp, ".", ext))
}

# 1. Correlation Matrix
cor_matrix <- cor(merged_data_no_geom[, vars], use = "complete.obs")
cor_plot <- corrplot(cor_matrix, 
                    method = "color",
                    type = "upper",
                    order = "hclust",
                    addCoef.col = "black",
                    number.cex = 0.7,
                    tl.col = "black",
                    tl.srt = 45,
                    title = "Correlation Matrix of Variables")

# Save correlation plot
cor_file <- get_timestamp_filename("correlation_matrix", "png")
png(cor_file, width = 2400, height = 2400, res = 300)
corrplot(cor_matrix, 
         method = "color",
         type = "upper",
         order = "hclust",
         addCoef.col = "black",
         number.cex = 0.7,
         tl.col = "black",
         tl.srt = 45,
         title = "Correlation Matrix of Variables")
dev.off()
print(paste("Saved correlation matrix as:", cor_file))

# 2. GWPCA Loadings Plot
loadings_data <- as.data.frame(gwpca_result$loadings)
loadings_data$Variable <- rownames(loadings_data)

loadings_plot <- ggplot(loadings_data, aes(x = reorder(Variable, PC1), y = PC1)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "GWPCA Component 1 Loadings",
       x = "Variables",
       y = "Loading Value") +
  theme(axis.text.y = element_text(size = 10))

# Save GWPCA loadings plot
loadings_file <- get_timestamp_filename("gwpca_loadings", "png")
ggsave(loadings_file, loadings_plot, width = 12, height = 8, dpi = 300)
print(paste("Saved GWPCA loadings as:", loadings_file))

# 3. Variance Explained Plot
var_exp <- data.frame(
  Component = paste0("PC", 1:3),
  Variance = gwpca_result$var[1:3]
)

variance_plot <- ggplot(var_exp, aes(x = Component, y = Variance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(title = "Variance Explained by GWPCA Components",
       y = "Variance Explained (%)") +
  theme(axis.text = element_text(size = 10))

# Save variance plot
variance_file <- get_timestamp_filename("variance_explained", "png")
ggsave(variance_file, variance_plot, width = 10, height = 6, dpi = 300)
print(paste("Saved variance plot as:", variance_file))

# 4. HVI Map
hvi_map <- ggplot(gwpca_sf) +
  geom_sf(aes(fill = HVI)) +
  scale_fill_viridis(
    option = "magma",
    name = "Heat Vulnerability\nIndex",
    limits = c(-3, 3),
    breaks = seq(-3, 3, by = 1)
  ) +
  theme_minimal() +
  labs(
    title = "Heat Vulnerability Index in Johannesburg",
    subtitle = "Standardized scores (z-scale)",
    caption = "Data source: Spatial analysis of heat vulnerability"
  ) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.position = "right",
    panel.grid = element_blank(),
    panel.border = element_rect(fill = NA, color = "grey50")
  )

# Save HVI map
hvi_file <- get_timestamp_filename("hvi_map", "png")
ggsave(hvi_file, hvi_map, width = 12, height = 8, dpi = 300)
print(paste("Saved HVI map as:", hvi_file))

# 5. LISA Cluster Map
lisa_map <- ggplot(gwpca_sf) +
  geom_sf(aes(fill = LISA_cluster)) +
  scale_fill_manual(
    values = c(
      "High-High" = "#d73027",
      "Low-Low" = "#4575b4",
      "High-Low" = "#fc8d59",
      "Low-High" = "#91bfdb",
      "Not Significant" = "#f7f7f7"
    ),
    name = "LISA Clusters"
  ) +
  theme_minimal() +
  labs(
    title = "Spatial Clustering of Heat Vulnerability in Johannesburg",
    subtitle = paste0(
      "Global Moran's I p-value: ", 
      format.pval(moran.test(gwpca_sf$HVI, weights_list)$p.value, digits = 3)
    ),
    caption = "Method: Local Indicators of Spatial Association (LISA)"
  ) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.position = "right",
    panel.grid = element_blank(),
    panel.border = element_rect(fill = NA, color = "grey50")
  )

# Save LISA map
lisa_file <- get_timestamp_filename("lisa_clusters", "png")
ggsave(lisa_file, lisa_map, width = 12, height = 8, dpi = 300)
print(paste("Saved LISA map as:", lisa_file))

# Create summary report
summary_file <- get_timestamp_filename("analysis_summary", "txt")
sink(summary_file)
cat("Heat Vulnerability Analysis Summary\n")
cat("=================================\n\n")

cat("1. GWPCA Results\n")
cat("--------------\n")
cat("Variance explained by components:\n")
print(var_exp)

cat("\n2. LISA Cluster Summary\n")
cat("--------------------\n")
print(table(gwpca_sf$LISA_cluster))

cat("\n3. HVI Summary Statistics\n")
cat("----------------------\n")
print(summary(gwpca_sf$HVI))

cat("\n4. Global Moran's I Test\n")
cat("---------------------\n")
print(moran.test(gwpca_sf$HVI, weights_list))

sink()
print(paste("Saved analysis summary as:", summary_file))
```

```{r spatial-analysis, cache=TRUE, cache.path='cache/', dependson=c('gwpca')}
# Calculate coordinates and neighbors
coords <- st_coordinates(st_centroid(gwpca_sf))
nb <- dnearneigh(coords, 0, 10000)
weights_list <- nb2listw(nb, style = "W")

# Calculate Global Moran's I using numeric HVI
global_moran <- moran.test(gwpca_sf$HVI, weights_list)

# Calculate local Moran's I
local_moran <- localmoran(gwpca_sf$HVI, weights_list)

# Calculate mean HVI
hvi_mean <- mean(gwpca_sf$HVI, na.rm = TRUE)
hvi_lag <- lag.listw(weights_list, gwpca_sf$HVI)

# Create LISA categories with proper indexing
gwpca_sf <- gwpca_sf %>%
  mutate(
    LISA_cluster = case_when(
      HVI > hvi_mean & hvi_lag > hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "High-High",
      HVI < hvi_mean & hvi_lag < hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "Low-Low",
      HVI > hvi_mean & hvi_lag < hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "High-Low",
      HVI < hvi_mean & hvi_lag > hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "Low-High",
      TRUE ~ "Not Significant"
    )
  )

# Print cluster summary
print("LISA Cluster Summary:")
print(table(gwpca_sf$LISA_cluster))

# Save Results
results_list <- list(
  gwpca_sf = gwpca_sf,
  global_moran = global_moran,
  local_moran = local_moran,
  variance_explained = variance_explained
)

saveRDS(results_list, "hvi_spatial_results.rds")