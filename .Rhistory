library(spdep)
library(RColorBrewer)
library(factoextra)
library(mice)
install.packages("mice")
# Load required libraries
library(GWmodel)
library(sp)
library(sf)
library(dplyr)
library(ggplot2)
library(viridis)
library(corrplot)
library(rlang)
library(psych)
library(tidyr)
library(spdep)
library(RColorBrewer)
library(factoextra)
library(mice)
# Read data
data <- read.csv('data.csv', header = TRUE)
geometry <- st_read("geometry.shp")
geometry <- st_transform(geometry, crs = 32735)
# Ensure consistent IDs
data$WardID_ <- as.character(data$WardID_)
geometry$WardID_ <- as.character(geometry$WardID_)
# Define variables
vars <- c(
"Crowded.dwellings", "No.piped.water", "Using.public.healthcare.facilities",
"Poor.health.status", "Failed.to.find.healthcare.when.needed",
"No.medical.insurance", "Household.hunger.risk",
"Benefiting.from.school.feeding.scheme", "UTFVI", "LST", "NDVI",
"NDBI__mean", "concern_he", "cancer_pro", "diabetes_p", "pneumonia_",
"heart_dise", "hypertensi", "hiv_prop", "tb_prop", "covid_prop",
"X60_plus_pr"
)
# Merge datasets
merged_data <- merge(geometry[, c("WardID_", "geometry")], data, by = "WardID_", all.x = TRUE)
# Handle missing data using MICE
merged_data_no_geom <- st_drop_geometry(merged_data)
imputed_data <- mice(merged_data_no_geom[vars], m = 5, maxit = 50)
merged_data_complete <- complete(imputed_data)
# Standardize variables
for (var in vars) {
merged_data[[var]] <- as.numeric(as.character(merged_data[[var]]))
merged_data[[var]] <- scale(merged_data[[var]])
}
# Compute summary statistics
summary_stats <- merged_data_no_geom %>%
select(all_of(vars)) %>%
summarise(across(everything(), list(
mean = ~mean(., na.rm = TRUE),
sd = ~sd(., na.rm = TRUE),
min = ~min(., na.rm = TRUE),
q25 = ~quantile(., 0.25, na.rm = TRUE),
median = ~median(., na.rm = TRUE),
q75 = ~quantile(., 0.75, na.rm = TRUE),
max = ~max(., na.rm = TRUE)
)))
# Display summary statistics
knitr::kable(t(summary_stats), digits = 2)
# Test for normality
shapiro_results <- lapply(merged_data_no_geom[vars], shapiro.test)
normality_summary <- data.frame(
Variable = vars,
Shapiro_p_value = sapply(shapiro_results, function(x) x$p.value)
)
# Display normality test results
knitr::kable(normality_summary, digits = 3)
# Compute both Pearson and Spearman correlations
pearson_corr <- cor(merged_data_no_geom[vars],
use = "pairwise.complete.obs",
method = "pearson")
spearman_corr <- cor(merged_data_no_geom[vars],
use = "pairwise.complete.obs",
method = "spearman")
# Plot correlation matrices
par(mfrow = c(1, 2))
corrplot(pearson_corr, method = "color", type = "upper",
order = "hclust",
tl.cex = 0.4, tl.col = "black", tl.srt = 45,
addCoef.col = "black", number.cex = 0.3,
title = "Pearson Correlation Matrix")
corrplot(spearman_corr, method = "color", type = "upper",
order = "hclust",
tl.cex = 0.4, tl.col = "black", tl.srt = 45,
addCoef.col = "black", number.cex = 0.3,
title = "Spearman Correlation Matrix")
# Compute both Pearson and Spearman correlations
pearson_corr <- cor(merged_data_no_geom[vars],
use = "pairwise.complete.obs",
method = "pearson")
spearman_corr <- cor(merged_data_no_geom[vars],
use = "pairwise.complete.obs",
method = "spearman")
# Plot correlation matrices
par(mfrow = c(1, 2))
corrplot(pearson_corr, method = "color", type = "upper",
order = "hclust",
tl.cex = 0.4, tl.col = "black", tl.srt = 45,
addCoef.col = "black", number.cex = 0.3,
title = "Pearson Correlation Matrix")
corrplot(spearman_corr, method = "color", type = "upper",
order = "hclust",
tl.cex = 0.4, tl.col = "black", tl.srt = 45,
addCoef.col = "black", number.cex = 0.3,
title = "Spearman Correlation Matrix")
# Convert to Spatial object
cleaned_data_sp <- as(merged_data, "Spatial")
# Estimate bandwidth
bw <- bw.gwpca(
data = cleaned_data_sp,
vars = vars,
k = 3,
robust = TRUE,
adaptive = TRUE
)
# Perform GWPCA
gwpca_result <- gwpca(
data = cleaned_data_sp,
vars = vars,
k = 3,
robust = TRUE,
bw = bw,
adaptive = TRUE,
scores = TRUE
)
# Bootstrap for confidence intervals
n_boot <- 1000
variance_boot <- matrix(NA, nrow = n_boot, ncol = 3)
for(i in 1:n_boot) {
boot_idx <- sample(1:nrow(merged_data), replace = TRUE)
boot_pca <- prcomp(merged_data_no_geom[boot_idx, vars], scale. = TRUE)
variance_boot[i,] <- boot_pca$sdev[1:3]^2 / sum(boot_pca$sdev^2) * 100
}
variance_ci <- apply(variance_boot, 2, quantile, c(0.025, 0.975))
# Display results
print("Variance Explained with 95% CI:")
print(variance_ci)
# Calculate HVI
gwpca_sf <- st_as_sf(gwpca_result$SDF)
variance_explained <- gwpca_result$var.exp
weights <- variance_explained[1:3] / sum(variance_explained[1:3])
gwpca_sf$HVI <- scale(
weights[1] * gwpca_sf$PC1 +
weights[2] * gwpca_sf$PC2 +
weights[3] * gwpca_sf$PC3
)
# Calculate Global Moran's I
coords <- st_coordinates(st_centroid(gwpca_sf))
nb <- dnearneigh(coords, 0, 10000)
weights <- nb2listw(nb, style = "W")
global_moran <- moran.test(gwpca_sf$HVI, weights)
# Convert GWPCA results to SF object
gwpca_sf <- st_as_sf(gwpca_result$SDF)
# Extract variance explained
variance_explained <- gwpca_result$var.exp[1:3]
weights <- variance_explained / sum(variance_explained)
# Calculate HVI - fixing the numeric vector issue
gwpca_sf <- gwpca_sf %>%
mutate(
PC1_weighted = as.numeric(gwpca_sf$PC1) * weights[1],
PC2_weighted = as.numeric(gwpca_sf$PC2) * weights[2],
PC3_weighted = as.numeric(gwpca_sf$PC3) * weights[3],
HVI = scale(PC1_weighted + PC2_weighted + PC3_weighted)
)
# Calculate HVI - fixing the size issue
# First check the structure of gwpca results
print("Checking GWPCA structure:")
print(names(gwpca_result))
print(dim(gwpca_result$SDF))
# Extract PC scores properly
gwpca_sf <- st_as_sf(gwpca_result$SDF)
scores_matrix <- matrix(NA, nrow = nrow(gwpca_sf), ncol = 3)
# Extract scores for each location
for(i in 1:nrow(gwpca_sf)) {
if(!is.null(gwpca_result$gwpca.scores[[i]])) {
scores_matrix[i, ] <- gwpca_result$gwpca.scores[[i]][1, 1:3]
}
}
# Add scores to gwpca_sf
gwpca_sf$PC1 <- scores_matrix[, 1]
gwpca_sf$PC2 <- scores_matrix[, 2]
gwpca_sf$PC3 <- scores_matrix[, 3]
# Calculate weights
variance_explained <- gwpca_result$var.exp[1:3]
weights <- variance_explained / sum(variance_explained)
# Calculate HVI with weighted components
gwpca_sf <- gwpca_sf %>%
mutate(
PC1_weighted = as.numeric(PC1) * weights[1],
PC2_weighted = as.numeric(PC2) * weights[2],
PC3_weighted = as.numeric(PC3) * weights[3]
)
# Calculate final HVI
gwpca_sf$HVI <- scale(gwpca_sf$PC1_weighted +
gwpca_sf$PC2_weighted +
gwpca_sf$PC3_weighted)
# Verify HVI calculation
print("HVI Summary:")
print(summary(gwpca_sf$HVI))
# Plot HVI
ggplot(gwpca_sf) +
geom_sf(aes(fill = HVI)) +
scale_fill_viridis() +
theme_minimal() +
labs(title = "Heat Vulnerability Index")
# Calculate coordinates and neighbors
coords <- st_coordinates(st_centroid(gwpca_sf))
nb <- dnearneigh(coords, 0, 10000)
weights_list <- nb2listw(nb, style = "W")
# Calculate Global Moran's I using numeric HVI
global_moran <- moran.test(gwpca_sf$HVI, weights_list)
# HVI Calculation - Fixing NA values issue
# First create empty matrices for scores
n_locs <- nrow(gwpca_result$SDF)
scores <- matrix(0, nrow = n_locs, ncol = 3)
colnames(scores) <- c("PC1", "PC2", "PC3")
# Extract scores properly from GWPCA results
for(i in 1:n_locs) {
if(!is.null(gwpca_result$gwpca.scores[[i]])) {
scores[i,] <- gwpca_result$gwpca.scores[[i]][1, 1:3]
}
}
# Create SF object and add PC scores
gwpca_sf <- st_as_sf(gwpca_result$SDF)
gwpca_sf$PC1 <- scores[,1]
gwpca_sf$PC2 <- scores[,2]
gwpca_sf$PC3 <- scores[,3]
# Calculate variance explained and weights
variance_explained <- gwpca_result$var.exp[1:3]
total_variance <- sum(variance_explained)
weights <- variance_explained/total_variance
# Print check of weights
print("Weights for PCs:")
print(weights)
# Calculate weighted components
gwpca_sf <- gwpca_sf %>%
mutate(
PC1_w = PC1 * weights[1],
PC2_w = PC2 * weights[2],
PC3_w = PC3 * weights[3],
HVI_raw = PC1_w + PC2_w + PC3_w
)
# Scale HVI to 0-1 range
gwpca_sf$HVI <- scale(gwpca_sf$HVI_raw)
# Check results
print("Summary of HVI components:")
print(summary(gwpca_sf %>% select(PC1_w, PC2_w, PC3_w, HVI_raw, HVI)))
# Plot HVI with proper color scaling
ggplot(gwpca_sf) +
geom_sf(aes(fill = HVI)) +
scale_fill_viridis(option = "magma",
name = "HVI",
guide = guide_colorbar(
title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme_minimal() +
labs(title = "Heat Vulnerability Index",
subtitle = "Standardized scores (z-scale)") +
theme(legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5))
# Save processed data
saveRDS(gwpca_sf, "gwpca_sf_with_hvi.rds")
View(gwpca_result)
# HVI Calculation - Fixing score extraction
# First extract the scores from gwpca_result directly
n_locs <- length(gwpca_result$gwpca.scores)
scores_matrix <- matrix(NA, nrow = n_locs, ncol = 3)
colnames(scores_matrix) <- c("PC1", "PC2", "PC3")
# Extract scores from each location's matrix
for(i in 1:n_locs) {
if(!is.null(gwpca_result$gwpca.scores[[i]])) {
# Each element contains a matrix - take the first row for the scores
scores_matrix[i,] <- as.numeric(gwpca_result$gwpca.scores[[i]][1, 1:3])
}
}
# Print diagnostics for scores
print("Summary of extracted PC scores:")
print(summary(as.data.frame(scores_matrix)))
# Create SF object with scores
gwpca_sf <- st_as_sf(gwpca_result$SDF)
gwpca_sf$PC1 <- scores_matrix[,1]
gwpca_sf$PC2 <- scores_matrix[,2]
gwpca_sf$PC3 <- scores_matrix[,3]
# Get variance explained from the GWPCA results
var_exp <- gwpca_result$var[1:3]
total_var <- sum(var_exp)
weights <- var_exp/total_var
print("Component weights:")
print(weights)
# Calculate weighted components
gwpca_sf <- gwpca_sf %>%
mutate(
PC1_weighted = PC1 * weights[1],
PC2_weighted = PC2 * weights[2],
PC3_weighted = PC3 * weights[3]
)
# Calculate HVI
gwpca_sf <- gwpca_sf %>%
mutate(
HVI_raw = PC1_weighted + PC2_weighted + PC3_weighted,
HVI = scale(HVI_raw)[,1]  # Explicitly take first column of scale output
)
# Check results
print("Summary of HVI calculation:")
print(summary(gwpca_sf %>%
select(PC1_weighted, PC2_weighted, PC3_weighted, HVI_raw, HVI)))
# Plot HVI
ggplot(gwpca_sf) +
geom_sf(aes(fill = HVI)) +
scale_fill_viridis(
option = "magma",
name = "Heat Vulnerability\nIndex",
limits = c(-3, 3)  # Set symmetric limits for z-scores
) +
theme_minimal() +
labs(
title = "Heat Vulnerability Index",
subtitle = "Standardized scores (z-scale)"
) +
theme(
plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5),
legend.position = "right"
)
# Save the results
saveRDS(gwpca_sf, "gwpca_sf_with_hvi.rds")
# Calculate coordinates and neighbors
coords <- st_coordinates(st_centroid(gwpca_sf))
nb <- dnearneigh(coords, 0, 10000)
weights_list <- nb2listw(nb, style = "W")
# Calculate Global Moran's I using numeric HVI
global_moran <- moran.test(gwpca_sf$HVI, weights_list)
# Calculate Local Moran's I
local_moran <- localmoran(gwpca_sf$HVI, weights_list)
# Calculate mean HVI
hvi_mean <- mean(gwpca_sf$HVI, na.rm = TRUE)
# Create LISA categories with proper indexing
gwpca_sf <- gwpca_sf %>%
mutate(LISA_cluster = case_when(
HVI > hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "High-High",
HVI < hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "Low-Low",
HVI > hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "High-Low",
HVI < hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "Low-High",
TRUE ~ "Not Significant"
))
# Plot LISA clusters
ggplot(gwpca_sf) +
geom_sf(aes(fill = LISA_cluster)) +
scale_fill_manual(values = c(
"High-High" = "red",
"Low-Low" = "blue",
"High-Low" = "pink",
"Low-High" = "lightblue",
"Not Significant" = "grey"
)) +
theme_minimal() +
labs(title = "LISA Cluster Map")
# Save Results
results_list <- list(
gwpca_sf = gwpca_sf,
global_moran = global_moran,
local_moran = local_moran,
variance_explained = variance_explained
)
saveRDS(results_list, "hvi_spatial_results.rds")
# Calculate updated correlation matrix using Spearman
# First ensure we have all required variables
key_vars <- data.frame(
LST = merged_data_no_geom$LST,
NDVI = merged_data_no_geom$NDVI,
Healthcare_Access = merged_data_no_geom$Using.public.healthcare.facilities,
Poverty_Index = merged_data_no_geom$Household.hunger.risk
)
# Calculate Spearman correlations
spearman_cor <- cor(key_vars, method = "spearman", use = "pairwise.complete.obs")
# Calculate p-values for correlations
spearman_p <- matrix(NA, 4, 4)
for(i in 1:4) {
for(j in 1:4) {
test <- cor.test(key_vars[,i], key_vars[,j],
method = "spearman",
exact = FALSE)
spearman_p[i,j] <- test$p.value
}
}
# Format correlation matrix for table
table5_matrix <- round(spearman_cor, 2)
rownames(table5_matrix) <- c("LST", "NDVI", "Healthcare Access", "Poverty Index")
colnames(table5_matrix) <- c("LST", "NDVI", "Healthcare Access", "Poverty Index")
# Add significance stars
table5_matrix_with_stars <- matrix("", 4, 4)
for(i in 1:4) {
for(j in 1:4) {
stars <- if(spearman_p[i,j] < 0.001) "***" else if(spearman_p[i,j] < 0.01) "**" else if(spearman_p[i,j] < 0.05) "*" else ""
table5_matrix_with_stars[i,j] <- paste0(table5_matrix[i,j], stars)
}
}
# Print formatted table
print("Table 5: Correlation Matrix of Key Variables (Spearman's rho)")
print(table5_matrix_with_stars)
# Save table to CSV
write.csv(table5_matrix_with_stars, "table5_spearman.csv")
# Print correlations with p-values for paper text
cat("\nKey correlations for paper text:\n")
cat("LST & NDVI: r =", table5_matrix[1,2], ", p <", format.pval(spearman_p[1,2], digits = 3), "\n")
cat("LST & Healthcare Access: r =", table5_matrix[1,3], ", p <", format.pval(spearman_p[1,3], digits = 3), "\n")
cat("LST & Poverty Index: r =", table5_matrix[1,4], ", p <", format.pval(spearman_p[1,4], digits = 3), "\n")
cat("Healthcare Access & Poverty Index: r =", table5_matrix[3,4], ", p <", format.pval(spearman_p[3,4], digits = 3), "\n")
# Identify wards with highest and lowest public healthcare access
healthcare_extremes <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
arrange(Using.public.healthcare.facilities) %>%
slice(c(1, n())) # Get first (lowest) and last (highest) rows
# Get top and bottom 5 for context
healthcare_top5 <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
arrange(desc(Using.public.healthcare.facilities)) %>%
slice_head(n = 5)
healthcare_bottom5 <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
arrange(Using.public.healthcare.facilities) %>%
slice_head(n = 5)
# Print results
print("Wards with extreme public healthcare facility usage:")
print(healthcare_extremes)
print("\nTop 5 wards with highest public healthcare usage:")
print(healthcare_top5)
print("\nBottom 5 wards with lowest public healthcare usage:")
print(healthcare_bottom5)
# Identify wards with healthcare extremes showing actual percentages
healthcare_summary <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
mutate(
percentage = Using.public.healthcare.facilities * 100  # Convert to percentage if needed
) %>%
arrange(desc(percentage))
# Get top 5
cat("Top 5 wards with highest public healthcare usage:\n")
print(head(healthcare_summary, 5))
# Get bottom 5
cat("\nBottom 5 wards with lowest public healthcare usage:\n")
print(tail(healthcare_summary, 5))
# Calculate overall statistics
cat("\nOverall Statistics:\n")
cat("Mean usage:", mean(healthcare_summary$percentage), "%\n")
cat("Standard deviation:", sd(healthcare_summary$percentage), "%\n")
# Get actual percentages for healthcare access
healthcare_summary <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
# Use raw values, not standardized scores
arrange(desc(Using.public.healthcare.facilities))
# Top 5 wards
cat("Top 5 wards with highest public healthcare usage:\n")
healthcare_top5 <- healthcare_summary %>%
head(5)
print(healthcare_top5)
# Bottom 5 wards
cat("\nBottom 5 wards with lowest public healthcare usage:\n")
healthcare_bottom5 <- healthcare_summary %>%
tail(5)
print(healthcare_bottom5)
# Calculate overall statistics
cat("\nOverall Statistics:\n")
cat(sprintf("Mean usage: %.2f%%\n",
mean(healthcare_summary$Using.public.healthcare.facilities)))
cat(sprintf("Standard deviation: %.2f%%\n",
sd(healthcare_summary$Using.public.healthcare.facilities)))
cat(sprintf("Range: %.2f%% to %.2f%%\n",
min(healthcare_summary$Using.public.healthcare.facilities),
max(healthcare_summary$Using.public.healthcare.facilities)))
# Add area names if available
# merged_data$ward_name or similar would be needed
# Get raw percentages for healthcare access
# First unstandardize the data if needed
healthcare_summary <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
mutate(
percentage = Using.public.healthcare.facilities * 100,  # Convert to percentage
WardID_ = as.character(WardID_)
) %>%
arrange(desc(percentage))
# Display top 5 highest usage wards
cat("\nTop 5 wards with highest public healthcare usage:\n")
healthcare_top5 <- healthcare_summary %>%
slice_head(n = 5) %>%
mutate(percentage = round(percentage, 2))
print(healthcare_top5)
# Display bottom 5 lowest usage wards
cat("\nBottom 5 wards with lowest public healthcare usage:\n")
healthcare_bottom5 <- healthcare_summary %>%
slice_tail(n = 5) %>%
mutate(percentage = round(percentage, 2))
print(healthcare_bottom5)
# Calculate summary statistics
cat("\nSummary Statistics:\n")
cat(sprintf("Mean usage: %.2f%%\n", mean(healthcare_summary$percentage)))
cat(sprintf("Standard deviation: %.2f%%\n", sd(healthcare_summary$percentage)))
cat(sprintf("Range: %.2f%% to %.2f%%\n",
min(healthcare_summary$percentage),
max(healthcare_summary$percentage)))
# Create a boxplot to visualize the distribution
ggplot(healthcare_summary, aes(y = percentage)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Public Healthcare Usage Across Wards",
y = "Percentage of Households Using Public Healthcare") +
theme(plot.title = element_text(hjust = 0.5))
