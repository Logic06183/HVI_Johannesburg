print(dim(gwpca_result$SDF))
# Extract PC scores properly
gwpca_sf <- st_as_sf(gwpca_result$SDF)
scores_matrix <- matrix(NA, nrow = nrow(gwpca_sf), ncol = 3)
# Extract scores for each location
for(i in 1:nrow(gwpca_sf)) {
if(!is.null(gwpca_result$gwpca.scores[[i]])) {
scores_matrix[i, ] <- gwpca_result$gwpca.scores[[i]][1, 1:3]
}
}
# Add scores to gwpca_sf
gwpca_sf$PC1 <- scores_matrix[, 1]
gwpca_sf$PC2 <- scores_matrix[, 2]
gwpca_sf$PC3 <- scores_matrix[, 3]
# Calculate weights
variance_explained <- gwpca_result$var.exp[1:3]
weights <- variance_explained / sum(variance_explained)
# Calculate HVI with weighted components
gwpca_sf <- gwpca_sf %>%
mutate(
PC1_weighted = as.numeric(PC1) * weights[1],
PC2_weighted = as.numeric(PC2) * weights[2],
PC3_weighted = as.numeric(PC3) * weights[3]
)
# Calculate final HVI
gwpca_sf$HVI <- scale(gwpca_sf$PC1_weighted +
gwpca_sf$PC2_weighted +
gwpca_sf$PC3_weighted)
# Verify HVI calculation
print("HVI Summary:")
print(summary(gwpca_sf$HVI))
# Plot HVI
ggplot(gwpca_sf) +
geom_sf(aes(fill = HVI)) +
scale_fill_viridis() +
theme_minimal() +
labs(title = "Heat Vulnerability Index")
# Calculate coordinates and neighbors
coords <- st_coordinates(st_centroid(gwpca_sf))
nb <- dnearneigh(coords, 0, 10000)
weights_list <- nb2listw(nb, style = "W")
# Calculate Global Moran's I using numeric HVI
global_moran <- moran.test(gwpca_sf$HVI, weights_list)
# HVI Calculation - Fixing NA values issue
# First create empty matrices for scores
n_locs <- nrow(gwpca_result$SDF)
scores <- matrix(0, nrow = n_locs, ncol = 3)
colnames(scores) <- c("PC1", "PC2", "PC3")
# Extract scores properly from GWPCA results
for(i in 1:n_locs) {
if(!is.null(gwpca_result$gwpca.scores[[i]])) {
scores[i,] <- gwpca_result$gwpca.scores[[i]][1, 1:3]
}
}
# Create SF object and add PC scores
gwpca_sf <- st_as_sf(gwpca_result$SDF)
gwpca_sf$PC1 <- scores[,1]
gwpca_sf$PC2 <- scores[,2]
gwpca_sf$PC3 <- scores[,3]
# Calculate variance explained and weights
variance_explained <- gwpca_result$var.exp[1:3]
total_variance <- sum(variance_explained)
weights <- variance_explained/total_variance
# Print check of weights
print("Weights for PCs:")
print(weights)
# Calculate weighted components
gwpca_sf <- gwpca_sf %>%
mutate(
PC1_w = PC1 * weights[1],
PC2_w = PC2 * weights[2],
PC3_w = PC3 * weights[3],
HVI_raw = PC1_w + PC2_w + PC3_w
)
# Scale HVI to 0-1 range
gwpca_sf$HVI <- scale(gwpca_sf$HVI_raw)
# Check results
print("Summary of HVI components:")
print(summary(gwpca_sf %>% select(PC1_w, PC2_w, PC3_w, HVI_raw, HVI)))
# Plot HVI with proper color scaling
ggplot(gwpca_sf) +
geom_sf(aes(fill = HVI)) +
scale_fill_viridis(option = "magma",
name = "HVI",
guide = guide_colorbar(
title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme_minimal() +
labs(title = "Heat Vulnerability Index",
subtitle = "Standardized scores (z-scale)") +
theme(legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5))
# Save processed data
saveRDS(gwpca_sf, "gwpca_sf_with_hvi.rds")
View(gwpca_result)
# HVI Calculation - Fixing score extraction
# First extract the scores from gwpca_result directly
n_locs <- length(gwpca_result$gwpca.scores)
scores_matrix <- matrix(NA, nrow = n_locs, ncol = 3)
colnames(scores_matrix) <- c("PC1", "PC2", "PC3")
# Extract scores from each location's matrix
for(i in 1:n_locs) {
if(!is.null(gwpca_result$gwpca.scores[[i]])) {
# Each element contains a matrix - take the first row for the scores
scores_matrix[i,] <- as.numeric(gwpca_result$gwpca.scores[[i]][1, 1:3])
}
}
# Print diagnostics for scores
print("Summary of extracted PC scores:")
print(summary(as.data.frame(scores_matrix)))
# Create SF object with scores
gwpca_sf <- st_as_sf(gwpca_result$SDF)
gwpca_sf$PC1 <- scores_matrix[,1]
gwpca_sf$PC2 <- scores_matrix[,2]
gwpca_sf$PC3 <- scores_matrix[,3]
# Get variance explained from the GWPCA results
var_exp <- gwpca_result$var[1:3]
total_var <- sum(var_exp)
weights <- var_exp/total_var
print("Component weights:")
print(weights)
# Calculate weighted components
gwpca_sf <- gwpca_sf %>%
mutate(
PC1_weighted = PC1 * weights[1],
PC2_weighted = PC2 * weights[2],
PC3_weighted = PC3 * weights[3]
)
# Calculate HVI
gwpca_sf <- gwpca_sf %>%
mutate(
HVI_raw = PC1_weighted + PC2_weighted + PC3_weighted,
HVI = scale(HVI_raw)[,1]  # Explicitly take first column of scale output
)
# Check results
print("Summary of HVI calculation:")
print(summary(gwpca_sf %>%
select(PC1_weighted, PC2_weighted, PC3_weighted, HVI_raw, HVI)))
# Plot HVI
ggplot(gwpca_sf) +
geom_sf(aes(fill = HVI)) +
scale_fill_viridis(
option = "magma",
name = "Heat Vulnerability\nIndex",
limits = c(-3, 3)  # Set symmetric limits for z-scores
) +
theme_minimal() +
labs(
title = "Heat Vulnerability Index",
subtitle = "Standardized scores (z-scale)"
) +
theme(
plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5),
legend.position = "right"
)
# Save the results
saveRDS(gwpca_sf, "gwpca_sf_with_hvi.rds")
# Calculate coordinates and neighbors
coords <- st_coordinates(st_centroid(gwpca_sf))
nb <- dnearneigh(coords, 0, 10000)
weights_list <- nb2listw(nb, style = "W")
# Calculate Global Moran's I using numeric HVI
global_moran <- moran.test(gwpca_sf$HVI, weights_list)
# Calculate Local Moran's I
local_moran <- localmoran(gwpca_sf$HVI, weights_list)
# Calculate mean HVI
hvi_mean <- mean(gwpca_sf$HVI, na.rm = TRUE)
# Create LISA categories with proper indexing
gwpca_sf <- gwpca_sf %>%
mutate(LISA_cluster = case_when(
HVI > hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "High-High",
HVI < hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "Low-Low",
HVI > hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "High-Low",
HVI < hvi_mean & local_moran[, "Pr(z != E(Ii))"] < 0.05 ~ "Low-High",
TRUE ~ "Not Significant"
))
# Plot LISA clusters
ggplot(gwpca_sf) +
geom_sf(aes(fill = LISA_cluster)) +
scale_fill_manual(values = c(
"High-High" = "red",
"Low-Low" = "blue",
"High-Low" = "pink",
"Low-High" = "lightblue",
"Not Significant" = "grey"
)) +
theme_minimal() +
labs(title = "LISA Cluster Map")
# Save Results
results_list <- list(
gwpca_sf = gwpca_sf,
global_moran = global_moran,
local_moran = local_moran,
variance_explained = variance_explained
)
saveRDS(results_list, "hvi_spatial_results.rds")
# Calculate updated correlation matrix using Spearman
# First ensure we have all required variables
key_vars <- data.frame(
LST = merged_data_no_geom$LST,
NDVI = merged_data_no_geom$NDVI,
Healthcare_Access = merged_data_no_geom$Using.public.healthcare.facilities,
Poverty_Index = merged_data_no_geom$Household.hunger.risk
)
# Calculate Spearman correlations
spearman_cor <- cor(key_vars, method = "spearman", use = "pairwise.complete.obs")
# Calculate p-values for correlations
spearman_p <- matrix(NA, 4, 4)
for(i in 1:4) {
for(j in 1:4) {
test <- cor.test(key_vars[,i], key_vars[,j],
method = "spearman",
exact = FALSE)
spearman_p[i,j] <- test$p.value
}
}
# Format correlation matrix for table
table5_matrix <- round(spearman_cor, 2)
rownames(table5_matrix) <- c("LST", "NDVI", "Healthcare Access", "Poverty Index")
colnames(table5_matrix) <- c("LST", "NDVI", "Healthcare Access", "Poverty Index")
# Add significance stars
table5_matrix_with_stars <- matrix("", 4, 4)
for(i in 1:4) {
for(j in 1:4) {
stars <- if(spearman_p[i,j] < 0.001) "***" else if(spearman_p[i,j] < 0.01) "**" else if(spearman_p[i,j] < 0.05) "*" else ""
table5_matrix_with_stars[i,j] <- paste0(table5_matrix[i,j], stars)
}
}
# Print formatted table
print("Table 5: Correlation Matrix of Key Variables (Spearman's rho)")
print(table5_matrix_with_stars)
# Save table to CSV
write.csv(table5_matrix_with_stars, "table5_spearman.csv")
# Print correlations with p-values for paper text
cat("\nKey correlations for paper text:\n")
cat("LST & NDVI: r =", table5_matrix[1,2], ", p <", format.pval(spearman_p[1,2], digits = 3), "\n")
cat("LST & Healthcare Access: r =", table5_matrix[1,3], ", p <", format.pval(spearman_p[1,3], digits = 3), "\n")
cat("LST & Poverty Index: r =", table5_matrix[1,4], ", p <", format.pval(spearman_p[1,4], digits = 3), "\n")
cat("Healthcare Access & Poverty Index: r =", table5_matrix[3,4], ", p <", format.pval(spearman_p[3,4], digits = 3), "\n")
# Identify wards with highest and lowest public healthcare access
healthcare_extremes <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
arrange(Using.public.healthcare.facilities) %>%
slice(c(1, n())) # Get first (lowest) and last (highest) rows
# Get top and bottom 5 for context
healthcare_top5 <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
arrange(desc(Using.public.healthcare.facilities)) %>%
slice_head(n = 5)
healthcare_bottom5 <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
arrange(Using.public.healthcare.facilities) %>%
slice_head(n = 5)
# Print results
print("Wards with extreme public healthcare facility usage:")
print(healthcare_extremes)
print("\nTop 5 wards with highest public healthcare usage:")
print(healthcare_top5)
print("\nBottom 5 wards with lowest public healthcare usage:")
print(healthcare_bottom5)
# Identify wards with healthcare extremes showing actual percentages
healthcare_summary <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
mutate(
percentage = Using.public.healthcare.facilities * 100  # Convert to percentage if needed
) %>%
arrange(desc(percentage))
# Get top 5
cat("Top 5 wards with highest public healthcare usage:\n")
print(head(healthcare_summary, 5))
# Get bottom 5
cat("\nBottom 5 wards with lowest public healthcare usage:\n")
print(tail(healthcare_summary, 5))
# Calculate overall statistics
cat("\nOverall Statistics:\n")
cat("Mean usage:", mean(healthcare_summary$percentage), "%\n")
cat("Standard deviation:", sd(healthcare_summary$percentage), "%\n")
# Get actual percentages for healthcare access
healthcare_summary <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
# Use raw values, not standardized scores
arrange(desc(Using.public.healthcare.facilities))
# Top 5 wards
cat("Top 5 wards with highest public healthcare usage:\n")
healthcare_top5 <- healthcare_summary %>%
head(5)
print(healthcare_top5)
# Bottom 5 wards
cat("\nBottom 5 wards with lowest public healthcare usage:\n")
healthcare_bottom5 <- healthcare_summary %>%
tail(5)
print(healthcare_bottom5)
# Calculate overall statistics
cat("\nOverall Statistics:\n")
cat(sprintf("Mean usage: %.2f%%\n",
mean(healthcare_summary$Using.public.healthcare.facilities)))
cat(sprintf("Standard deviation: %.2f%%\n",
sd(healthcare_summary$Using.public.healthcare.facilities)))
cat(sprintf("Range: %.2f%% to %.2f%%\n",
min(healthcare_summary$Using.public.healthcare.facilities),
max(healthcare_summary$Using.public.healthcare.facilities)))
# Add area names if available
# merged_data$ward_name or similar would be needed
# Get raw percentages for healthcare access
# First unstandardize the data if needed
healthcare_summary <- merged_data %>%
select(WardID_, Using.public.healthcare.facilities) %>%
mutate(
percentage = Using.public.healthcare.facilities * 100,  # Convert to percentage
WardID_ = as.character(WardID_)
) %>%
arrange(desc(percentage))
# Display top 5 highest usage wards
cat("\nTop 5 wards with highest public healthcare usage:\n")
healthcare_top5 <- healthcare_summary %>%
slice_head(n = 5) %>%
mutate(percentage = round(percentage, 2))
print(healthcare_top5)
# Display bottom 5 lowest usage wards
cat("\nBottom 5 wards with lowest public healthcare usage:\n")
healthcare_bottom5 <- healthcare_summary %>%
slice_tail(n = 5) %>%
mutate(percentage = round(percentage, 2))
print(healthcare_bottom5)
# Calculate summary statistics
cat("\nSummary Statistics:\n")
cat(sprintf("Mean usage: %.2f%%\n", mean(healthcare_summary$percentage)))
cat(sprintf("Standard deviation: %.2f%%\n", sd(healthcare_summary$percentage)))
cat(sprintf("Range: %.2f%% to %.2f%%\n",
min(healthcare_summary$percentage),
max(healthcare_summary$percentage)))
# Create a boxplot to visualize the distribution
ggplot(healthcare_summary, aes(y = percentage)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Public Healthcare Usage Across Wards",
y = "Percentage of Households Using Public Healthcare") +
theme(plot.title = element_text(hjust = 0.5))
# First, let's properly prepare the data
hvi_values <- as.vector(merged_data_clean$HVI_normalized)
View(merged_data)
View(merged_data_complete)
# Load necessary libraries
library(sf)            # For handling spatial data
library(sp)            # Spatial data classes
library(dplyr)         # Data manipulation
library(ggplot2)       # Data visualization
library(corrplot)      # Correlation plots
library(psych)         # PCA and psychometric analysis
library(GWmodel)       # Geographically Weighted Models
library(spdep)         # Spatial dependence analysis
library(factoextra)    # Visualization for PCA
library(spgwr)         # Geographically Weighted Regression
library(spData)        # Sample spatial data
library(RColorBrewer)  # Color palettes
library(viridis)       # Color palettes
# Set working directory
setwd("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg")
# Read data
data <- read.csv('data.csv', header = TRUE)
geometry <- st_read("geometry.shp")
geometry <- st_transform(geometry, crs = 32735)
# Load necessary libraries
library(sf)            # For handling spatial data
library(sp)            # Spatial data classes
library(dplyr)         # Data manipulation
library(ggplot2)       # Data visualization
library(corrplot)      # Correlation plots
library(psych)         # PCA and psychometric analysis
library(GWmodel)       # Geographically Weighted Models
library(spdep)         # Spatial dependence analysis
library(factoextra)    # Visualization for PCA
library(spgwr)         # Geographically Weighted Regression
library(spData)        # Sample spatial data
library(RColorBrewer)  # Color palettes
library(viridis)       # Color palettes
library(tidyr)         # Data reshaping (for pivot_longer)
# Set working directory
setwd("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg")
# Read data
data <- read.csv('data.csv', header = TRUE)
geometry <- st_read("geometry.shp")
geometry <- st_transform(geometry, crs = 32735)
# Ensure consistent IDs (replace 'WardID_' with your actual key column)
data$WardID_ <- as.character(data$WardID_)
geometry$WardID_ <- as.character(geometry$WardID_)
# Merge datasets
merged_data <- merge(
geometry[, c("WardID_", "geometry")],
data,
by = "WardID_",
all.x = TRUE
)
# Define variables
vars <- c(
"Crowded.dwellings", "No.piped.water", "Using.public.healthcare.facilities",
"Poor.health.status", "Failed.to.find.healthcare.when.needed",
"No.medical.insurance", "Household.hunger.risk",
"Benefiting.from.school.feeding.scheme", "UTFVI", "LST", "NDVI",
"NDBI__mean", "concern_he", "cancer_pro", "diabetes_p", "pneumonia_",
"heart_dise", "hypertensi", "hiv_prop", "tb_prop", "covid_prop",
"X60_plus_pr"
)
# Check for missing variables
missing_vars <- setdiff(vars, names(merged_data))
if (length(missing_vars) > 0) {
stop("Missing variables: ", paste(missing_vars, collapse = ", "))
}
# Ensure variables are numeric
for (var in vars) {
if (!is.numeric(merged_data[[var]])) {
merged_data[[var]] <- as.numeric(as.character(merged_data[[var]]))
}
}
# Remove rows with missing data
merged_data_no_geom <- st_drop_geometry(merged_data)
rows_complete <- complete.cases(merged_data_no_geom[vars])
merged_data <- merged_data[rows_complete, ]
merged_data_no_geom <- merged_data_no_geom[rows_complete, ]
# Compute descriptive statistics
summary_stats <- merged_data_no_geom %>%
select(all_of(vars)) %>%
summarise(across(everything(), list(
mean = ~mean(.),
sd = ~sd(.),
median = ~median(.),
min = ~min(.),
max = ~max(.)
)))
# Reshape to long format using names_pattern
summary_stats_long <- summary_stats %>%
pivot_longer(
cols = everything(),
names_to = c("Variable", "Statistic"),
names_pattern = "^(.*)_(mean|sd|median|min|max)$"
)
# View the summary statistics
print(summary_stats_long)
# Compute correlation matrix
corr_matrix <- cor(
merged_data_no_geom %>% select(all_of(vars)),
use = "complete.obs"
)
# Visualize correlation matrix
corrplot(corr_matrix, method = "circle")
# Compute summary statistics
summary_stats <- gwpca_sf %>%
st_set_geometry(NULL) %>%
summarise(across(all_of(vars), list(mean = mean, sd = sd, median = median, min = min, max = max)))
# First, let's properly prepare the data
hvi_values <- as.vector(merged_data_clean$HVI_normalized)
View(boot_pca)
View(cleaned_data_sp)
# Print summary of clusters
print(table(gwpca_sf$LISA_cluster))
# Load required libraries
library(spdep)
library(sf)
library(ggplot2)
# Load the saved GWPCA results that contain your HVI values
gwpca_sf <- readRDS("gwpca_sf_with_hvi.rds")
# Get the HVI values
hvi_values <- as.vector(gwpca_sf$HVI)
# Create spatial weights
coords <- st_coordinates(st_centroid(gwpca_sf))
nb <- dnearneigh(coords, 0, 10000)  # Using 10km distance as in your original code
w <- nb2listw(nb, style="W")
# Calculate Local Moran's I
local_moran <- localmoran(hvi_values, w)
# Add results back to our spatial dataframe
gwpca_sf$LocalI <- local_moran[, "Ii"]
gwpca_sf$LocalI_P <- local_moran[, "Pr(z != E(Ii))"]
hvi_lag <- lag.listw(w, hvi_values)
# Calculate mean for high/low classification
hvi_mean <- mean(hvi_values)
# Create LISA cluster categories with p-value information
gwpca_sf$LISA_cluster <- NA
gwpca_sf$LISA_cluster[hvi_values > hvi_mean &
hvi_lag > hvi_mean &
gwpca_sf$LocalI_P < 0.05] <- "High-High (p < 0.05)"
gwpca_sf$LISA_cluster[hvi_values < hvi_mean &
hvi_lag < hvi_mean &
gwpca_sf$LocalI_P < 0.05] <- "Low-Low (p < 0.05)"
gwpca_sf$LISA_cluster[hvi_values > hvi_mean &
hvi_lag < hvi_mean &
gwpca_sf$LocalI_P < 0.05] <- "High-Low (p < 0.05)"
gwpca_sf$LISA_cluster[hvi_values < hvi_mean &
hvi_lag > hvi_mean &
gwpca_sf$LocalI_P < 0.05] <- "Low-High (p < 0.05)"
gwpca_sf$LISA_cluster[gwpca_sf$LocalI_P >= 0.05] <- "Not Significant (p ≥ 0.05)"
# Create LISA map
ggplot(gwpca_sf) +
geom_sf(aes(fill = LISA_cluster)) +
scale_fill_manual(values = c("High-High (p < 0.05)" = "red",
"Low-Low (p < 0.05)" = "blue",
"High-Low (p < 0.05)" = "pink",
"Low-High (p < 0.05)" = "lightblue",
"Not Significant (p ≥ 0.05)" = "white")) +
theme_minimal() +
labs(title = "LISA Clusters for Heat Vulnerability",
subtitle = paste("Global Moran's I p-value:", format.pval(moran.test(hvi_values, w)$p.value, digits = 3)),
fill = "Cluster Type") +
theme(plot.title = element_text(size = 14, face = "bold"),
legend.position = "right",
legend.text = element_text(size = 8))
# Print Global Moran's I
print(moran.test(hvi_values, w))
# Print summary of clusters
print(table(gwpca_sf$LISA_cluster))
