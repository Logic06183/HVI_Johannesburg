# Print the result
print(top_10_vulnerable_wards)
# Plot the top 10 most vulnerable wards
plot_top_10_hvi <- ggplot(top_10_vulnerable_wards) +
geom_sf(aes(fill = HVI)) +
scale_fill_viridis() +
theme_minimal() +
labs(title = "Top 10 Most Vulnerable Wards in Johannesburg", fill = "HVI")
print(plot_top_10_hvi)
# Plot the top 10 most vulnerable wards
plot_top_10_hvi <- ggplot(top_10_vulnerable_wards) +
geom_sf(aes(fill = HVI)) +
scale_fill_viridis() +
theme_minimal() +
labs(title = "Top 10 Most Vulnerable Wards in Johannesburg", fill = "HVI")
print(plot_top_10_hvi)
# Plot the entire map of Johannesburg
plot_overlay <- ggplot() +
# Plot all wards with a base color
geom_sf(data = gwpca_sf, aes(geometry = geometry), fill = "gray80", color = "white", lwd = 0.2) +
# Overlay top 10 vulnerable wards with a distinct color
geom_sf(data = top_10_vulnerable_wards, aes(geometry = geometry, fill = HVI), color = "red", lwd = 0.8) +
# Apply a color scale for the top 10 wards based on their HVI
scale_fill_viridis(option = "C") +
theme_minimal() +
labs(title = "Top 10 Most Vulnerable Wards in Johannesburg Overlay",
fill = "HVI Score") +
theme(legend.position = "bottom")
# Print the overlay plot
print(plot_overlay)
# Extract loadings for the first principal component
loadings_matrix <- gwpca_result$loadings
# Check the loadings for PC1
pc1_loadings <- loadings_matrix[, 1]
loadings_matrix <- as.matrix(gwpca_result$loadings)
# Extract loadings for the first principal component
loadings_matrix <- gwpca_result$loadings
# Check the loadings for PC1
pc1_loadings <- loadings_matrix[, 1]
loadings_matrix <- as.matrix(gwpca_result$loadings)
# Check the structure of loadings_matrix
str(gwpca_result$loadings)
# If loadings_matrix is not already a matrix, convert it
loadings_matrix <- as.matrix(gwpca_result$loadings)
# Verify if it has at least two dimensions
if (length(dim(loadings_matrix)) == 2) {
# Extract the loadings for the first principal component
pc1_loadings <- loadings_matrix[, 1]
} else {
stop("Loadings matrix does not have the correct number of dimensions.")
# Check the structure of loadings_matrix
str(gwpca_result$loadings)
# If loadings_matrix is not already a matrix, convert it
loadings_matrix <- as.matrix(gwpca_result$loadings)
# Verify if it has at least two dimensions
if (length(dim(loadings_matrix)) == 2) {
# Extract the loadings for the first principal component
pc1_loadings <- loadings_matrix[, 1]
} else {
stop("Loadings matrix does not have the correct number of dimensions.")
}
# Check the structure of loadings_matrix
str(gwpca_result$loadings)
# If loadings_matrix is not already a matrix, convert it
loadings_matrix <- as.matrix(gwpca_result$loadings)
# Verify if it has at least two dimensions
if (length(dim(loadings_matrix)) == 2) {
# Extract the loadings for the first principal component
pc1_loadings <- loadings_matrix[, 1]
} else {
stop("Loadings matrix does not have the correct number of dimensions.")
}
# Extract loadings for the first principal component
loadings_matrix <- gwpca_result$loadings
# Check the loadings for PC1
pc1_loadings <- loadings_matrix[, 1]
# Create a data frame of variables and their loadings for PC1
loadings_df <- data.frame(Variable = rownames(loadings_matrix), PC1_Loading = pc1_loadings)
# Sort the loadings by absolute value to find the most contributing variables
loadings_df <- loadings_df %>%
mutate(Absolute_Loading = abs(PC1_Loading)) %>%
arrange(desc(Absolute_Loading))
# Display the top contributing variables to PC1
top_contributing_variables <- loadings_df[1:10, ]
print("Top contributing variables to PC1:")
print(top_contributing_variables)
# Assuming loadings_matrix already exists from previous calculations
# Ensure loadings_matrix has variable names as row names
rownames(loadings_matrix) <- vars
# Assuming loadings_matrix already exists from previous calculations
# Ensure loadings_matrix has variable names as row names
rownames(loadings_matrix) <- vars
source("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg/GWPCA.R")
# Extract loadings for the first principal component
pc1_loadings <- loadings_matrix[, 1]
# Create a data frame of variables and their loadings for PC1
loadings_df <- data.frame(Variable = rownames(loadings_matrix), PC1_Loading = pc1_loadings)
# Extract loadings for the first principal component
pc1_loadings <- loadings_matrix[, 1]
# Create a data frame of variables and their loadings for PC1
loadings_df <- data.frame(Variable = rownames(loadings_matrix), PC1_Loading = pc1_loadings)
# Create a data frame of variables and their loadings for PC1
loadings_df <- data.frame(Variable = rownames(loadings_matrix), PC1_Loading = pc1_loadings)
# Extract loadings for the first principal component
loadings_matrix <- as.matrix(gwpca_result$loadings)
# Ensure the length of vars matches the number of rows in loadings_matrix
if (nrow(loadings_matrix) == length(vars)) {
rownames(loadings_matrix) <- vars
} else {
stop("Mismatch between the number of rows in loadings_matrix and the length of vars.")
}
# Extract loadings for the first principal component
loadings_matrix <- as.matrix(gwpca_result$loadings)
View(loadings_matrix)
# Verify score columns
print("Columns in gwpca_sf after adding scores:")
print(names(gwpca_sf))
# Sort the data by HVI in descending order and select the top 10 most vulnerable wards
top_10_vulnerable_wards <- gwpca_sf %>%
arrange(desc(HVI)) %>%
slice(1:10)
# Print the result
print(top_10_vulnerable_wards)
# Plot the top 10 most vulnerable wards
plot_top_10_hvi <- ggplot(top_10_vulnerable_wards) +
geom_sf(aes(fill = HVI)) +
scale_fill_viridis() +
theme_minimal() +
labs(title = "Top 10 Most Vulnerable Wards in Johannesburg", fill = "HVI")
print(plot_top_10_hvi)
# Extract loadings for the first principal component
loadings_matrix <- as.matrix(gwpca_result$loadings)
View(loadings_matrix)
View(local_scores)
View(loadings_matrix)
View(local_scores)
# Ensure the length of vars matches the number of rows in loadings_matrix
if (nrow(loadings_matrix) == length(vars)) {
rownames(loadings_matrix) <- vars
} else {
stop("Mismatch between the number of rows in loadings_matrix and the length of vars.")
}
# Extract loadings for the first principal component
loadings_matrix <- as.matrix(gwpca_result$loadings)
# Print the dimensions to debug
cat("Dimensions of loadings_matrix:", dim(loadings_matrix), "\n")
cat("Number of variables in vars:", length(vars), "\n")
# Ensure the length of vars matches the number of rows in loadings_matrix
if (nrow(loadings_matrix) == length(vars)) {
rownames(loadings_matrix) <- vars
} else {
stop("Mismatch between the number of rows in loadings_matrix and the length of vars. Please verify your GWPCA calculation.")
}
# Extract loadings for the first principal component
pc1_loadings <- loadings_matrix[, 1]
# Create a data frame of variables and their loadings for PC1
loadings_df <- data.frame(Variable = rownames(loadings_matrix), PC1_Loading = pc1_loadings)
# Sort the loadings by absolute value to find the most contributing variables
loadings_df <- loadings_df %>%
mutate(Absolute_Loading = abs(PC1_Loading)) %>%
arrange(desc(Absolute_Loading))
View(loadings_df)
View(gwpca_result)
# Extract loadings for the first principal component
loadings_matrix <- gwpca_result$loadings[, , 1]  # Extract the loadings for PC1 (third dimension index = 1)
# Ensure the length of vars matches the number of rows in loadings_matrix
if (nrow(loadings_matrix) == length(vars)) {
rownames(loadings_matrix) <- vars
} else {
stop("Mismatch between the number of rows in loadings_matrix and the length of vars.")
}
source("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg/GWPCA.R")
# Extract loadings array
loadings_array <- gwpca_result$loadings  # Dimensions: variables x components x locations
# Compute the mean loadings across all locations
mean_loadings <- apply(loadings_array, c(1,2), mean, na.rm = TRUE)  # Result is variables x components
# Ensure the length of vars matches the number of rows in mean_loadings
if (nrow(mean_loadings) == length(vars)) {
rownames(mean_loadings) <- vars
} else {
stop("Mismatch between the number of rows in mean_loadings and the length of vars.")
}
# Extract loadings array
loadings_array <- gwpca_result$loadings  # Dimensions: variables x components x locations
# Check the dimensions
dims <- dim(loadings_array)
cat("Dimensions of loadings_array:", dims, "\n")
# Compute the mean loadings across all locations
mean_loadings <- apply(loadings_array, c(1,2), mean, na.rm = TRUE)  # Expected to be variables x components
# Check the dimensions
mean_dims <- dim(mean_loadings)
cat("Dimensions of mean_loadings:", mean_dims, "\n")
length_vars <- length(vars)
cat("Length of vars:", length_vars, "\n")
if (nrow(mean_loadings) == length(vars)) {
cat("The number of rows in mean_loadings matches the length of vars.\n")
} else {
cat("Mismatch detected:\n")
cat("Number of rows in mean_loadings:", nrow(mean_loadings), "\n")
cat("Length of vars:", length(vars), "\n")
}
# Get variable names from loadings_array
variables_in_loadings <- dimnames(loadings_array)[[1]]
cat("Variables in loadings_array:\n")
print(variables_in_loadings)
missing_in_loadings <- setdiff(vars, variables_in_loadings)
extra_in_loadings <- setdiff(variables_in_loadings, vars)
cat("Variables in vars but not in loadings_array:\n")
print(missing_in_loadings)
cat("Variables in loadings_array but not in vars:\n")
print(extra_in_loadings)
# Extract loadings array
loadings_array <- gwpca_result$loadings  # Dimensions: locations x variables x components
# Compute the mean loadings across all locations
mean_loadings <- apply(loadings_array, c(2,3), mean, na.rm = TRUE)  # Result is variables x components
# Check the dimensions
mean_dims <- dim(mean_loadings)
cat("Dimensions of mean_loadings:", mean_dims, "\n")
# Ensure the length of vars matches the number of rows in mean_loadings
if (nrow(mean_loadings) == length(vars)) {
rownames(mean_loadings) <- vars
} else {
stop("Mismatch between the number of rows in mean_loadings and the length of vars.")
}
# Extract loadings for the first principal component
pc1_loadings <- mean_loadings[, 1]
# Create a data frame of variables and their loadings for PC1
loadings_df <- data.frame(Variable = rownames(mean_loadings), PC1_Loading = pc1_loadings)
# Sort the loadings by absolute value to find the most contributing variables
loadings_df <- loadings_df %>%
mutate(Absolute_Loading = abs(PC1_Loading)) %>%
arrange(desc(Absolute_Loading))
# Display the top contributing variables to PC1
top_contributing_variables <- loadings_df[1:10, ]
# Print the top contributing variables in a cleaner format
cat("Top contributing variables to PC1:\n")
print(top_contributing_variables, row.names = FALSE)
source("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg/GWPCA.R")
# Compute descriptive statistics
summary_stats <- merged_data_no_geom %>%
select(all_of(vars)) %>%
summarise_all(list(
mean = ~mean(.),
sd = ~sd(.),
median = ~median(.),
min = ~min(.),
max = ~max(.)
)) %>%
pivot_longer(everything(), names_to = c("Variable", ".value"), names_sep = "_")
# Compute correlation matrix
corr_matrix <- cor(merged_data_no_geom[vars], use = "complete.obs")
# Visualize correlation matrix
corrplot(corr_matrix, method = "color", type = "upper", order = "hclust",
tl.cex = 0.8, tl.col = "black", tl.srt = 45,
addCoef.col = "black", number.cex = 0.7,
title = "Correlation Matrix of Variables")
source("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg/GWPCA.R")
install.packages("psych")
source("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg/GWPCA.R")
install.packages("tidyr")
source("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg/GWPCA.R")
rlang::last_trace()
source("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg/GWPCA.R")
# Compute correlation matrix with a subset of variables (top 10 by variance)
vars_subset <- names(sort(apply(merged_data_no_geom[vars], 2, var), decreasing = TRUE))[1:10]
# Compute correlation matrix with a subset of variables (top 10 by variance)
vars_subset <- names(sort(apply(merged_data_no_geom[vars], 2, var), decreasing = TRUE))[1:10]
# Compute correlation matrix with a subset of variables (top 10 by variance)
# Ensure 'X60_plus_pr' is numeric and rename for safety
merged_data_no_geom$X60_plus_pr <- as.numeric(as.character(merged_data_no_geom$X60_plus_pr))
names(merged_data_no_geom)[names(merged_data_no_geom) == "X60_plus_pr"] <- "Age_60_plus_prop"
vars[vars == "X60_plus_pr"] <- "Age_60_plus_prop"
vars_subset <- names(sort(apply(merged_data_no_geom[vars], 2, var, na.rm = TRUE), decreasing = TRUE))[1:10]
# Load required libraries
library(GWmodel)
library(sp)
library(sf)
library(dplyr)
library(ggplot2)
library(viridis)
library(corrplot)
library(rlang)
library(psych)  # For descriptive statistics
library(tidyr)   # For pivot_longer function
# Set working directory
setwd("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg")
# Read data
data <- read.csv('data.csv', header = TRUE)
geometry <- st_read("geometry.shp")
geometry <- st_transform(geometry, crs = 32735)
# Ensure consistent IDs
data$WardID_ <- as.character(data$WardID_)
geometry$WardID_ <- as.character(geometry$WardID_)
# Merge datasets
merged_data <- merge(geometry[, c("WardID_", "geometry")], data, by = "WardID_", all.x = TRUE)
# Define variables
vars <- c(
"Crowded.dwellings", "No.piped.water", "Using.public.healthcare.facilities",
"Poor.health.status", "Failed.to.find.healthcare.when.needed",
"No.medical.insurance", "Household.hunger.risk",
"Benefiting.from.school.feeding.scheme", "UTFVI", "LST", "NDVI",
"NDBI__mean", "concern_he", "cancer_pro", "diabetes_p", "pneumonia_",
"heart_dise", "hypertensi", "hiv_prop", "tb_prop", "covid_prop",
"X60_plus_pr"
)
# Check for missing variables
missing_vars <- setdiff(vars, names(merged_data))
if (length(missing_vars) > 0) {
stop("Missing variables: ", paste(missing_vars, collapse = ", "))
}
# Ensure variables are numeric
for (var in vars) {
if (!is.numeric(merged_data[[var]])) {
merged_data[[var]] <- as.numeric(as.character(merged_data[[var]]))
}
}
# Remove rows with missing data
merged_data_no_geom <- st_drop_geometry(merged_data)
rows_complete <- complete.cases(merged_data_no_geom[vars])
merged_data <- merged_data[rows_complete, ]
merged_data_no_geom <- merged_data_no_geom[rows_complete, ]
# Compute descriptive statistics
summary_stats <- merged_data_no_geom %>%
select(all_of(vars)) %>%
summarise(across(everything(), list(
mean = ~mean(.),
sd = ~sd(.),
median = ~median(.),
min = ~min(.),
max = ~max(.)
)))
# Reshape to long format using names_pattern
summary_stats_long <- summary_stats %>%
pivot_longer(
cols = everything(),
names_to = c("Variable", "Statistic"),
names_pattern = "^(.*)_(mean|sd|median|min|max)$"
)
print("Summary Statistics:")
print(summary_stats_long)
# Compute correlation matrix
corr_matrix <- cor(merged_data_no_geom[vars], use = "complete.obs")
corrplot(corr_matrix, method = "color", type = "upper", order = "hclust",
tl.cex = 0.6, tl.col = "black", tl.srt = 45,
addCoef.col = "black", number.cex = 0.5,
title = "Correlation Matrix of Variables")
# Convert to Spatial object
cleaned_data_sp <- as(merged_data, "Spatial")
# Check CRS
if (is.na(proj4string(cleaned_data_sp))) {
stop("Spatial object has no CRS defined.")
} else {
cat("CRS of spatial data:", proj4string(cleaned_data_sp), "\n")
}
source("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg/GWPCA.R")
# Compute correlation matrix with a subset of variables (top 10 by variance)
# Ensure 'X60_plus_pr' is numeric and rename for safety
merged_data_no_geom$X60_plus_pr <- as.numeric(as.character(merged_data_no_geom$X60_plus_pr))
names(merged_data_no_geom)[names(merged_data_no_geom) == "X60_plus_pr"] <- "Age_60_plus_prop"
vars[vars == "X60_plus_pr"] <- "Age_60_plus_prop"
vars_subset <- names(sort(apply(merged_data_no_geom[vars], 2, var, na.rm = TRUE), decreasing = TRUE))[1:10]
# Load required libraries
library(GWmodel)
library(sp)
library(sf)
library(dplyr)
library(ggplot2)
library(viridis)
library(corrplot)
library(rlang)
library(psych)  # For descriptive statistics
library(tidyr)   # For pivot_longer function
# Set working directory
setwd("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg")
# Read data
data <- read.csv('data.csv', header = TRUE)
geometry <- st_read("geometry.shp")
geometry <- st_transform(geometry, crs = 32735)
# Ensure consistent IDs
data$WardID_ <- as.character(data$WardID_)
geometry$WardID_ <- as.character(geometry$WardID_)
# Merge datasets
merged_data <- merge(geometry[, c("WardID_", "geometry")], data, by = "WardID_", all.x = TRUE)
# Define variables
vars <- c(
"Crowded.dwellings", "No.piped.water", "Using.public.healthcare.facilities",
"Poor.health.status", "Failed.to.find.healthcare.when.needed",
"No.medical.insurance", "Household.hunger.risk",
"Benefiting.from.school.feeding.scheme", "UTFVI", "LST", "NDVI",
"NDBI__mean", "concern_he", "cancer_pro", "diabetes_p", "pneumonia_",
"heart_dise", "hypertensi", "hiv_prop", "tb_prop", "covid_prop",
"X60_plus_pr"
)
# Check for missing variables
missing_vars <- setdiff(vars, names(merged_data))
if (length(missing_vars) > 0) {
stop("Missing variables: ", paste(missing_vars, collapse = ", "))
}
# Ensure variables are numeric
for (var in vars) {
if (!is.numeric(merged_data[[var]])) {
merged_data[[var]] <- as.numeric(as.character(merged_data[[var]]))
}
}
# Remove rows with missing data
merged_data_no_geom <- st_drop_geometry(merged_data)
rows_complete <- complete.cases(merged_data_no_geom[vars])
merged_data <- merged_data[rows_complete, ]
merged_data_no_geom <- merged_data_no_geom[rows_complete, ]
# Compute descriptive statistics
summary_stats <- merged_data_no_geom %>%
select(all_of(vars)) %>%
summarise(across(everything(), list(
mean = ~mean(.),
sd = ~sd(.),
median = ~median(.),
min = ~min(.),
max = ~max(.)
)))
# Reshape to long format using names_pattern
summary_stats_long <- summary_stats %>%
pivot_longer(
cols = everything(),
names_to = c("Variable", "Statistic"),
names_pattern = "^(.*)_(mean|sd|median|min|max)$"
)
print("Summary Statistics:")
print(summary_stats_long)
# Compute correlation matrix with a subset of variables (top 10 by variance)
# Ensure 'X60_plus_pr' is numeric and rename for safety
merged_data_no_geom$X60_plus_pr <- as.numeric(as.character(merged_data_no_geom$X60_plus_pr))
names(merged_data_no_geom)[names(merged_data_no_geom) == "X60_plus_pr"] <- "Age_60_plus_prop"
vars[vars == "X60_plus_pr"] <- "Age_60_plus_prop"
vars_subset <- names(sort(apply(merged_data_no_geom[vars], 2, var, na.rm = TRUE), decreasing = TRUE))[1:10]
# Load required libraries
library(GWmodel)
library(sp)
library(sf)
library(dplyr)
library(ggplot2)
library(viridis)
library(corrplot)
library(rlang)
library(psych)  # For descriptive statistics
library(tidyr)   # For pivot_longer function
# Set working directory
setwd("C:/Users/CraigParker/OneDrive - Wits PHR/Desktop/HVI_Johannesburg")
# Read data
data <- read.csv('data.csv', header = TRUE)
geometry <- st_read("geometry.shp")
geometry <- st_transform(geometry, crs = 32735)
# Ensure consistent IDs
data$WardID_ <- as.character(data$WardID_)
geometry$WardID_ <- as.character(geometry$WardID_)
# Merge datasets
merged_data <- merge(geometry[, c("WardID_", "geometry")], data, by = "WardID_", all.x = TRUE)
# Define variables
vars <- c(
"Crowded.dwellings", "No.piped.water", "Using.public.healthcare.facilities",
"Poor.health.status", "Failed.to.find.healthcare.when.needed",
"No.medical.insurance", "Household.hunger.risk",
"Benefiting.from.school.feeding.scheme", "UTFVI", "LST", "NDVI",
"NDBI__mean", "concern_he", "cancer_pro", "diabetes_p", "pneumonia_",
"heart_dise", "hypertensi", "hiv_prop", "tb_prop", "covid_prop",
"X60_plus_pr"
)
# Check for missing variables
missing_vars <- setdiff(vars, names(merged_data))
if (length(missing_vars) > 0) {
stop("Missing variables: ", paste(missing_vars, collapse = ", "))
}
# Ensure variables are numeric
for (var in vars) {
if (!is.numeric(merged_data[[var]])) {
merged_data[[var]] <- as.numeric(as.character(merged_data[[var]]))
}
}
# Remove rows with missing data
merged_data_no_geom <- st_drop_geometry(merged_data)
rows_complete <- complete.cases(merged_data_no_geom[vars])
merged_data <- merged_data[rows_complete, ]
merged_data_no_geom <- merged_data_no_geom[rows_complete, ]
# Compute descriptive statistics
summary_stats <- merged_data_no_geom %>%
select(all_of(vars)) %>%
summarise(across(everything(), list(
mean = ~mean(.),
sd = ~sd(.),
median = ~median(.),
min = ~min(.),
max = ~max(.)
)))
# Reshape to long format using names_pattern
summary_stats_long <- summary_stats %>%
pivot_longer(
cols = everything(),
names_to = c("Variable", "Statistic"),
names_pattern = "^(.*)_(mean|sd|median|min|max)$"
)
print("Summary Statistics:")
print(summary_stats_long)
# Compute correlation matrix
corr_matrix <- cor(merged_data_no_geom[vars], use = "complete.obs")
corrplot(corr_matrix, method = "color", type = "upper", order = "hclust",
tl.cex = 0.6, tl.col = "black", tl.srt = 45,
addCoef.col = "black", number.cex = 0.5,
title = "Correlation Matrix of Variables")
# Adjusted correlation matrix plot for better readability
corrplot(corr_matrix, method = "color", type = "upper", order = "hclust",
tl.cex = 0.8, tl.col = "black", tl.srt = 90, # Increased text size and rotated labels for better visibility
addCoef.col = "black", number.cex = 0.6,
title = "Correlation Matrix of Top 10 Variables", mar = c(0, 0, 2, 0))
